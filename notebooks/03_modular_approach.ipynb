{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf56902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import tiktoken\n",
    "import hashlib\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class TextReader:\n",
    "    \n",
    "    def __init__(self, file_path, source, text_id, split_pattern=None) -> None:\n",
    "        self.file_path = Path(file_path)\n",
    "        self.source = source\n",
    "        self.text = \"\"\n",
    "        self.enc = tiktoken.get_encoding('cl100k_base')\n",
    "        self.pattern = split_pattern or r\"^(?:CHAPTER [IVXLCDM]+\\.)\\s*\\n\"\n",
    "        self.text_id = text_id \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def simple_hash(text: str, length: int = 7) -> str:\n",
    "        return hashlib.md5(text.encode(\"utf-8\")).hexdigest()[:length]\n",
    "\n",
    "    def _section_split(self):\n",
    "        return re.split(self.pattern, self.text, flags=re.IGNORECASE | re.MULTILINE)\n",
    "\n",
    "    def _chunk_split(self, section: str, max_tokens=1000, overlap=100):\n",
    "        tokens = self.enc.encode(section)\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), max_tokens - overlap):\n",
    "            chunk = self.enc.decode(tokens[i:i+max_tokens])\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def parse_into_chunks(self, max_tokens=800, overlap=100):\n",
    "        sections = self._section_split()\n",
    "        all_chunks = []\n",
    "        for sid, section in enumerate(sections): \n",
    "            section_chunks = self._chunk_split(section, max_tokens, overlap)\n",
    "            for cid, sub in enumerate(section_chunks):\n",
    "                hash_id = TextReader.simple_hash(sub)\n",
    "                all_chunks.append(\n",
    "                    {\n",
    "                        \"id\": f\"{self.text_id}_{sid+1:02d}_{cid+1:03d}_{hash_id}\",\n",
    "                        \"text\": sub,\n",
    "                        \"num_tokens\": len(self.enc.encode(sub)),\n",
    "                        \"num_chars\": len(sub)\n",
    "                    }\n",
    "                )\n",
    "        return all_chunks\n",
    "\n",
    "\n",
    "\n",
    "class GutenbergReader(TextReader):\n",
    "\n",
    "    def __init__(self, file_path, text_id) -> None:\n",
    "        super().__init__(file_path, text_id=text_id, source=\"gutenberg\")\n",
    "\n",
    "\n",
    "    def _strip_gutenberg(self, text: str) -> str:\n",
    "        start_match = re.search(r\"\\*\\*\\* START OF.*\\*\\*\\*\", text)\n",
    "        end_match = re.search(r\"\\*\\*\\* END OF.*\\*\\*\\*\", text)\n",
    "        if start_match and end_match:\n",
    "            return text[start_match.end(): end_match.start()]\n",
    "        return text\n",
    "    \n",
    "    def parse(self, max_tokens=500, overlap=100):\n",
    "        book_path = Path(self.file_path)\n",
    "        raw_text = book_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "        self.text = self._strip_gutenberg(raw_text)\n",
    "        print(\"Clean word count:\", len(self.text.split()))\n",
    "    \n",
    "        chunks = self.parse_into_chunks(max_tokens, overlap)\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec4ef77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean word count: 26525\n",
      "Num chunks: 99\n",
      "Preview:  this), “to go on crying in this way! Stop\n",
      "this moment, I tell you!” But she went on all the same, shedding\n",
      "gallons of tears, until there was a large pool all round her, about\n",
      "four inches deep and reaching half down the hall.\n",
      "\n",
      "After a time she heard a little pattering of feet in the distance, and\n",
      "sh\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = \"../DATA/alice_in_wonderland.txt\"\n",
    "reader = GutenbergReader(FILE_PATH, \"aiw\")\n",
    "chunks = reader.parse(max_tokens=500, overlap=100)\n",
    "\n",
    "print(\"Num chunks:\", len(chunks))\n",
    "print(\"Preview:\", chunks[10][\"text\"][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e511da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>num_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aiw_01_001_de2f1cf</td>\n",
       "      <td>\\n\\n[Illustration]\\n\\n\\n\\n\\nAlice’s Adventures...</td>\n",
       "      <td>159</td>\n",
       "      <td>586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aiw_02_001_174ec36</td>\n",
       "      <td>Down the Rabbit-Hole\\n\\n\\nAlice was beginning ...</td>\n",
       "      <td>500</td>\n",
       "      <td>2108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aiw_02_002_91bf310</td>\n",
       "      <td>well was very deep, or she fell very slowly, ...</td>\n",
       "      <td>500</td>\n",
       "      <td>1973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aiw_02_003_11c51f3</td>\n",
       "      <td>\\nidea what Latitude was, or Longitude either,...</td>\n",
       "      <td>500</td>\n",
       "      <td>1856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aiw_02_004_374f0bd</td>\n",
       "      <td>zing off, and had just begun to dream that she...</td>\n",
       "      <td>500</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                               text  \\\n",
       "0  aiw_01_001_de2f1cf  \\n\\n[Illustration]\\n\\n\\n\\n\\nAlice’s Adventures...   \n",
       "1  aiw_02_001_174ec36  Down the Rabbit-Hole\\n\\n\\nAlice was beginning ...   \n",
       "2  aiw_02_002_91bf310   well was very deep, or she fell very slowly, ...   \n",
       "3  aiw_02_003_11c51f3  \\nidea what Latitude was, or Longitude either,...   \n",
       "4  aiw_02_004_374f0bd  zing off, and had just begun to dream that she...   \n",
       "\n",
       "   num_tokens  num_chars  \n",
       "0         159        586  \n",
       "1         500       2108  \n",
       "2         500       1973  \n",
       "3         500       1856  \n",
       "4         500       2022  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(chunks, columns=['id', 'text', 'num_tokens', 'num_chars'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26eea64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df['num_chars'], bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e97904",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d89cafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "STOPWORDS = {\"the\",\"a\",\"an\",\"and\",\"of\",\"in\",\"to\"}\n",
    "def simple_tokenize(text):\n",
    "    return [w for w in re.findall(r\"\\w+\", text.lower()) if w not in STOPWORDS]\n",
    "\n",
    "\n",
    "class BM25Retriever:\n",
    "    def __init__(self, k1=1.5, b=0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.docs = []\n",
    "        self.doc_lens = []\n",
    "        self.avgdl = 0\n",
    "        self.df = {}\n",
    "        self.idf = {}\n",
    "        self.N = 0\n",
    "        self.ids = []\n",
    "        self.raw_docs = []\n",
    "\n",
    "    def build_index(self, chunks):\n",
    "        self.docs = []\n",
    "        self.ids = []\n",
    "        self.df = {}\n",
    "        self.raw_docs = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            tokens = simple_tokenize(chunk[\"text\"])\n",
    "            self.docs.append(tokens)\n",
    "            self.ids.append(chunk[\"id\"])\n",
    "            self.raw_docs.append(chunk[\"text\"])\n",
    "\n",
    "        self.N = len(self.docs)\n",
    "        self.doc_lens = [len(doc) for doc in self.docs]\n",
    "        self.avgdl = sum(self.doc_lens) / self.N if self.N > 0 else 0\n",
    "\n",
    "        for doc in self.docs:\n",
    "            for word in set(doc):\n",
    "                self.df[word] = self.df.get(word, 0) + 1\n",
    "\n",
    "        self.idf = {\n",
    "            word: math.log((self.N - freq + 0.5) / (freq + 0.5) + 1)\n",
    "            for word, freq in self.df.items()\n",
    "        }\n",
    "\n",
    "    def score(self, query_tokens, idx):\n",
    "        doc = self.docs[idx]\n",
    "        doc_len = self.doc_lens[idx]\n",
    "        freqs = Counter(doc)\n",
    "        score = 0.0\n",
    "        for term in query_tokens:\n",
    "            if term not in freqs:\n",
    "                continue\n",
    "            df = freqs[term]\n",
    "            idf = self.idf.get(term, 0)\n",
    "            numer = df * (self.k1 + 1)\n",
    "            denom = df + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)\n",
    "            score += idf * (numer / denom)\n",
    "        return score\n",
    "\n",
    "    def search(self, query, topk=7):\n",
    "        query_tokens = simple_tokenize(query)\n",
    "        scores = [self.score(query_tokens, i) for i in range(self.N)]\n",
    "        ranked = sorted(enumerate(scores), key=lambda x: -x[1])[:topk]\n",
    "        return [{\"id\": self.ids[i], \"text\": self.raw_docs[i], \"score\": s} for i, s in ranked]\n",
    "\n",
    "    def id_search(self, query: str, topk=7):\n",
    "        search_results = self.search(query, topk)\n",
    "        return [c['id'] for c in search_results]\n",
    "\n",
    "    def cleanup(self):\n",
    "        self.docs = []\n",
    "        self.ids = []\n",
    "        self.df = {}\n",
    "        self.idf = {}\n",
    "        self.doc_lens = []\n",
    "        self.avgdl = 0\n",
    "        self.N = 0\n",
    "        self.raw_docs = []\n",
    "        print(\"BM25 index cleared.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a212108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Retriever()\n",
    "bm25.build_index(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90376393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'aiw_02_001_174ec36',\n",
       "  'text': 'Down the Rabbit-Hole\\n\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into\\nthe book her sister was reading, but it had no pictures or\\nconversations in it, “and what is the use of a book,” thought Alice\\n“without pictures or conversations?”\\n\\nSo she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure of\\nmaking a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.\\n\\nThere was nothing so _very_ remarkable in that; nor did Alice think it\\nso _very_ much out of the way to hear the Rabbit say to itself, “Oh\\ndear! Oh dear! I shall be late!” (when she thought it over afterwards,\\nit occurred to her that she ought to have wondered at this, but at the\\ntime it all seemed quite natural); but when the Rabbit actually _took a\\nwatch out of its waistcoat-pocket_, and looked at it, and then hurried\\non, Alice started to her feet, for it flashed across her mind that she\\nhad never before seen a rabbit with either a waistcoat-pocket, or a\\nwatch to take out of it, and burning with curiosity, she ran across the\\nfield after it, and fortunately was just in time to see it pop down a\\nlarge rabbit-hole under the hedge.\\n\\nIn another moment down went Alice after it, never once considering how\\nin the world she was to get out again.\\n\\nThe rabbit-hole went straight on like a tunnel for some way, and then\\ndipped suddenly down, so suddenly that Alice had not a moment to think\\nabout stopping herself before she found herself falling down a very\\ndeep well.\\n\\nEither the well was very deep, or she fell very slowly, for she had\\nplenty of time as she went down to look about her and to wonder what\\nwas going to happen next. First, she tried to look down and make out\\nwhat she was coming to, but it was too dark to see anything; then she\\nlooked at the sides of the well, and noticed that they were filled with\\ncupboards and book-shelves; here and there she saw maps and pictures\\n',\n",
       "  'score': 2.7634718843461656},\n",
       " {'id': 'aiw_05_004_e6854cf',\n",
       "  'text': '_”\\n\\n“Oh, you foolish Alice!” she answered herself. “How can you learn\\nlessons in here? Why, there’s hardly room for _you_, and no room at all\\nfor any lesson-books!”\\n\\nAnd so she went on, taking first one side and then the other, and\\nmaking quite a conversation of it altogether; but after a few minutes\\nshe heard a voice outside, and stopped to listen.\\n\\n“Mary Ann! Mary Ann!” said the voice. “Fetch me my gloves this moment!”\\nThen came a little pattering of feet on the stairs. Alice knew it was\\nthe Rabbit coming to look for her, and she trembled till she shook the\\nhouse, quite forgetting that she was now about a thousand times as\\nlarge as the Rabbit, and had no reason to be afraid of it.\\n\\nPresently the Rabbit came up to the door, and tried to open it; but, as\\nthe door opened inwards, and Alice’s elbow was pressed hard against it,\\nthat attempt proved a failure. Alice heard it say to itself “Then I’ll\\ngo round and get in at the window.”\\n\\n“_That_ you won’t!” thought Alice, and, after waiting till she fancied\\nshe heard the Rabbit just under the window, she suddenly spread out her\\nhand, and made a snatch in the air. She did not get hold of anything,\\nbut she heard a little shriek and a fall, and a crash of broken glass,\\nfrom which she concluded that it was just possible it had fallen into a\\ncucumber-frame, or something of the sort.\\n\\nNext came an angry voice—the Rabbit’s—“Pat! Pat! Where are you?” And\\nthen a voice she had never heard before, “Sure then I’m here! Digging\\nfor apples, yer honour!”\\n\\n“Digging for apples, indeed!” said the Rabbit angrily. “Here! Come and\\nhelp me out of _this!_” (Sounds of more broken glass.)\\n\\n“Now tell me, Pat, what’s that in the window?”\\n\\n“Sure, it’s an arm, yer honour!” (He pronounced it “arrum.”)\\n\\n“An arm, you goose! Who ever saw one that size? Why, it fills the whole\\nwindow!”\\n\\n“Sure, it does, yer honour: but it’s an arm for all that.”\\n\\n“Well, it’s got no business there, at any rate: go and take it away!”\\n\\n',\n",
       "  'score': 2.704029637706801},\n",
       " {'id': 'aiw_13_003_0d652ea',\n",
       "  'text': ', please your Majesty,” said the\\nWhite Rabbit, jumping up in a great hurry; “this paper has just been\\npicked up.”\\n\\n“What’s in it?” said the Queen.\\n\\n“I haven’t opened it yet,” said the White Rabbit, “but it seems to be a\\nletter, written by the prisoner to—to somebody.”\\n\\n“It must have been that,” said the King, “unless it was written to\\nnobody, which isn’t usual, you know.”\\n\\n“Who is it directed to?” said one of the jurymen.\\n\\n“It isn’t directed at all,” said the White Rabbit; “in fact, there’s\\nnothing written on the _outside_.” He unfolded the paper as he spoke,\\nand added “It isn’t a letter, after all: it’s a set of verses.”\\n\\n“Are they in the prisoner’s handwriting?” asked another of the jurymen.\\n\\n“No, they’re not,” said the White Rabbit, “and that’s the queerest\\nthing about it.” (The jury all looked puzzled.)\\n\\n“He must have imitated somebody else’s hand,” said the King. (The jury\\nall brightened up again.)\\n\\n“Please your Majesty,” said the Knave, “I didn’t write it, and they\\ncan’t prove I did: there’s no name signed at the end.”\\n\\n“If you didn’t sign it,” said the King, “that only makes the matter\\nworse. You _must_ have meant some mischief, or else you’d have signed\\nyour name like an honest man.”\\n\\nThere was a general clapping of hands at this: it was the first really\\nclever thing the King had said that day.\\n\\n“That _proves_ his guilt,” said the Queen.\\n\\n“It proves nothing of the sort!” said Alice. “Why, you don’t even know\\nwhat they’re about!”\\n\\n“Read them,” said the King.\\n\\nThe White Rabbit put on his spectacles. “Where shall I begin, please\\nyour Majesty?” he asked.\\n\\n“Begin at the beginning,” the King said gravely, “and go on till you\\ncome to the end: then stop.”\\n\\nThese were the verses the White Rabbit read:—\\n\\n“They told me you had been to her,\\n    And mentioned me to him:\\nShe gave me a good character,\\n    But said I could not swim.\\n\\nHe sent them word I had not gone\\n    (We know it to be true):\\nIf she should push the matter',\n",
       "  'score': 2.6596055598936146},\n",
       " {'id': 'aiw_09_004_150d296',\n",
       "  'text': ' tone, going\\ndown on one knee as he spoke, “we were trying—”\\n\\n“_I_ see!” said the Queen, who had meanwhile been examining the roses.\\n“Off with their heads!” and the procession moved on, three of the\\nsoldiers remaining behind to execute the unfortunate gardeners, who ran\\nto Alice for protection.\\n\\n“You shan’t be beheaded!” said Alice, and she put them into a large\\nflower-pot that stood near. The three soldiers wandered about for a\\nminute or two, looking for them, and then quietly marched off after the\\nothers.\\n\\n“Are their heads off?” shouted the Queen.\\n\\n“Their heads are gone, if it please your Majesty!” the soldiers shouted\\nin reply.\\n\\n“That’s right!” shouted the Queen. “Can you play croquet?”\\n\\nThe soldiers were silent, and looked at Alice, as the question was\\nevidently meant for her.\\n\\n“Yes!” shouted Alice.\\n\\n“Come on, then!” roared the Queen, and Alice joined the procession,\\nwondering very much what would happen next.\\n\\n“It’s—it’s a very fine day!” said a timid voice at her side. She was\\nwalking by the White Rabbit, who was peeping anxiously into her face.\\n\\n“Very,” said Alice: “—where’s the Duchess?”\\n\\n“Hush! Hush!” said the Rabbit in a low, hurried tone. He looked\\nanxiously over his shoulder as he spoke, and then raised himself upon\\ntiptoe, put his mouth close to her ear, and whispered “She’s under\\nsentence of execution.”\\n\\n“What for?” said Alice.\\n\\n“Did you say ‘What a pity!’?” the Rabbit asked.\\n\\n“No, I didn’t,” said Alice: “I don’t think it’s at all a pity. I said\\n‘What for?’”\\n\\n“She boxed the Queen’s ears—” the Rabbit began. Alice gave a little\\nscream of laughter. “Oh, hush!” the Rabbit whispered in a frightened\\ntone. “The Queen will hear you! You see, she came rather late, and the\\nQueen said—”\\n\\n“Get to your places!” shouted the Queen in a voice of thunder, and\\npeople began running about in all directions, tumbling up against each\\nother; however, they got settled down in a minute or two, and the game\\nbegan. Alice thought she had never seen such a curious croquet',\n",
       "  'score': 2.630208929530794},\n",
       " {'id': 'aiw_05_001_e9978e9',\n",
       "  'text': 'The Rabbit Sends in a Little Bill\\n\\n\\nIt was the White Rabbit, trotting slowly back again, and looking\\nanxiously about as it went, as if it had lost something; and she heard\\nit muttering to itself “The Duchess! The Duchess! Oh my dear paws! Oh\\nmy fur and whiskers! She’ll get me executed, as sure as ferrets are\\nferrets! Where _can_ I have dropped them, I wonder?” Alice guessed in a\\nmoment that it was looking for the fan and the pair of white kid\\ngloves, and she very good-naturedly began hunting about for them, but\\nthey were nowhere to be seen—everything seemed to have changed since\\nher swim in the pool, and the great hall, with the glass table and the\\nlittle door, had vanished completely.\\n\\nVery soon the Rabbit noticed Alice, as she went hunting about, and\\ncalled out to her in an angry tone, “Why, Mary Ann, what _are_ you\\ndoing out here? Run home this moment, and fetch me a pair of gloves and\\na fan! Quick, now!” And Alice was so much frightened that she ran off\\nat once in the direction it pointed to, without trying to explain the\\nmistake it had made.\\n\\n“He took me for his housemaid,” she said to herself as she ran. “How\\nsurprised he’ll be when he finds out who I am! But I’d better take him\\nhis fan and gloves—that is, if I can find them.” As she said this, she\\ncame upon a neat little house, on the door of which was a bright brass\\nplate with the name “W. RABBIT,” engraved upon it. She went in without\\nknocking, and hurried upstairs, in great fear lest she should meet the\\nreal Mary Ann, and be turned out of the house before she had found the\\nfan and gloves.\\n\\n“How queer it seems,” Alice said to herself, “to be going messages for\\na rabbit! I suppose Dinah’ll be sending me on messages next!” And she\\nbegan fancying the sort of thing that would happen: “‘Miss Alice! Come\\nhere directly, and get ready for your walk!’ ‘Coming in a minute,\\nnurse! But I’ve got to see that the mouse doesn’t get out.’ Only I\\ndon’t think,” Alice went on,',\n",
       "  'score': 2.5966717761279665},\n",
       " {'id': 'aiw_01_001_de2f1cf',\n",
       "  'text': '\\n\\n[Illustration]\\n\\n\\n\\n\\nAlice’s Adventures in Wonderland\\n\\nby Lewis Carroll\\n\\nTHE MILLENNIUM FULCRUM EDITION 3.0\\n\\nContents\\n\\n CHAPTER I.     Down the Rabbit-Hole\\n CHAPTER II.    The Pool of Tears\\n CHAPTER III.   A Caucus-Race and a Long Tale\\n CHAPTER IV.    The Rabbit Sends in a Little Bill\\n CHAPTER V.     Advice from a Caterpillar\\n CHAPTER VI.    Pig and Pepper\\n CHAPTER VII.   A Mad Tea-Party\\n CHAPTER VIII.  The Queen’s Croquet-Ground\\n CHAPTER IX.    The Mock Turtle’s Story\\n CHAPTER X.     The Lobster Quadrille\\n CHAPTER XI.    Who Stole the Tarts?\\n CHAPTER XII.   Alice’s Evidence\\n\\n\\n\\n\\n',\n",
       "  'score': 2.5671938305604525},\n",
       " {'id': 'aiw_12_002_364d96b',\n",
       "  'text': ', “jury-men”\\nwould have done just as well.\\n\\nThe twelve jurors were all writing very busily on slates. “What are\\nthey doing?” Alice whispered to the Gryphon. “They can’t have anything\\nto put down yet, before the trial’s begun.”\\n\\n“They’re putting down their names,” the Gryphon whispered in reply,\\n“for fear they should forget them before the end of the trial.”\\n\\n“Stupid things!” Alice began in a loud, indignant voice, but she\\nstopped hastily, for the White Rabbit cried out, “Silence in the\\ncourt!” and the King put on his spectacles and looked anxiously round,\\nto make out who was talking.\\n\\nAlice could see, as well as if she were looking over their shoulders,\\nthat all the jurors were writing down “stupid things!” on their slates,\\nand she could even make out that one of them didn’t know how to spell\\n“stupid,” and that he had to ask his neighbour to tell him. “A nice\\nmuddle their slates’ll be in before the trial’s over!” thought Alice.\\n\\nOne of the jurors had a pencil that squeaked. This of course, Alice\\ncould _not_ stand, and she went round the court and got behind him, and\\nvery soon found an opportunity of taking it away. She did it so quickly\\nthat the poor little juror (it was Bill, the Lizard) could not make out\\nat all what had become of it; so, after hunting all about for it, he\\nwas obliged to write with one finger for the rest of the day; and this\\nwas of very little use, as it left no mark on the slate.\\n\\n“Herald, read the accusation!” said the King.\\n\\nOn this the White Rabbit blew three blasts on the trumpet, and then\\nunrolled the parchment scroll, and read as follows:—\\n\\n“The Queen of Hearts, she made some tarts,\\n    All on a summer day:\\nThe Knave of Hearts, he stole those tarts,\\n    And took them quite away!”\\n\\n\\n“Consider your verdict,” the King said to the jury.\\n\\n“Not yet, not yet!” the Rabbit hastily interrupted. “There’s a great\\ndeal to come before that!”\\n\\n“Call the first witness,” said the King; and the White Rabbit blew\\nthree blasts on the trumpet, and called out, “First witness!”\\n\\nThe first witness was the H',\n",
       "  'score': 2.4639879135546785}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.search(\"alice rabbit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b9e53f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'aiw_02_007_4b8f451',\n",
       " 'text': ' the table for it, she found she could not possibly reach\\nit: she could see it quite plainly through the glass, and she tried her\\nbest to climb up one of the legs of the table, but it was too slippery;\\nand when she had tired herself out with trying, the poor little thing\\nsat down and cried.\\n\\n“Come, there’s no use in crying like that!” said Alice to herself,\\nrather sharply; “I advise you to leave off this minute!” She generally\\ngave herself very good advice, (though she very seldom followed it),\\nand sometimes she scolded herself so severely as to bring tears into\\nher eyes; and once she remembered trying to box her own ears for having\\ncheated herself in a game of croquet she was playing against herself,\\nfor this curious child was very fond of pretending to be two people.\\n“But it’s no use now,” thought poor Alice, “to pretend to be two\\npeople! Why, there’s hardly enough of me left to make _one_ respectable\\nperson!”\\n\\nSoon her eye fell on a little glass box that was lying under the table:\\nshe opened it, and found in it a very small cake, on which the words\\n“EAT ME” were beautifully marked in currants. “Well, I’ll eat it,” said\\nAlice, “and if it makes me grow larger, I can reach the key; and if it\\nmakes me grow smaller, I can creep under the door; so either way I’ll\\nget into the garden, and I don’t care which happens!”\\n\\nShe ate a little bit, and said anxiously to herself, “Which way? Which\\nway?”, holding her hand on the top of her head to feel which way it was\\ngrowing, and she was quite surprised to find that she remained the same\\nsize: to be sure, this generally happens when one eats cake, but Alice\\nhad got so much into the way of expecting nothing but out-of-the-way\\nthings to happen, that it seemed quite dull and stupid for life to go\\non in the common way.\\n\\nSo she set to work, and very soon finished off the cake.\\n\\n*      *      *      *      *      *      *\\n\\n    *      *      *      *      *      *\\n\\n*      *      *      *      *      *      *\\n\\n\\n\\n\\n',\n",
       " 'num_tokens': 484,\n",
       " 'num_chars': 1977}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "924ede40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sethurama/DEV/LM/book-mate/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Golden data for testing - Preparation. \n",
    "\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "from string import Template\n",
    "\n",
    "gt_prompt_template = Template(\"\"\" \n",
    "You are emulating realistic book readers with diverse reading habits. \n",
    "The readers have mostly read the book and are now trying to recall, clarify, \n",
    "or explore details by searching within the book. \n",
    "Some are precise, some are vague, some type in full questions, \n",
    "and others use only a couple of words or short phrases.\n",
    "\n",
    "\n",
    "TASK:\n",
    "Formulate exactly 5 distinct search queries that a reader might ask \n",
    "based only on the following 'Record' (viz. a chunked information from the book/content).\n",
    "\n",
    "Record:\n",
    "\n",
    "id: $id\n",
    "text: $text\n",
    "num_tokens: $num_tokens\n",
    "num_chars: $num_chars\n",
    "                              \n",
    "HARD CONSTRAINTS:\n",
    "- Use ONLY details present in the record (no external knowledge).\n",
    "- Max 15 words per query.\n",
    "- Each query must sound like something a real book reader would type.\n",
    "- Ensure a MIX of query STYLES and INTENT across the 5:\n",
    "    • 1 short keyword-style query (2–4 words).  Occasionally, this may be purely keyword-based.\n",
    "    • 1 natural-language full question.\n",
    "    • 1 phrase-like query (fragment, not a full sentence).\n",
    "    • 1 detail-oriented recall query (who/what/where).\n",
    "    • 1 deeper reflective or interpretive query (asking about meaning, emotion, theme, or motivation).\n",
    "- Avoid reusing the same key noun/adjective across queries; vary wording and style.\n",
    "- At least one query must be **thoughtful and reflective** (not just factual or keyword-based).\n",
    "- Queries should sound like a human reader’s questions, not summaries.\n",
    "- Do NOT always output the 5 query types in the same sequence. Randomly vary the order \n",
    "  so the set looks natural and less templated.\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Return only a JSON array of 5 strings, with no extra text:\n",
    "[\n",
    "  \"query_1\",\n",
    "  \"query_2\",\n",
    "  \"query_3\",\n",
    "  \"query_4\",\n",
    "  \"query_5\"\n",
    "]\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "class GoldenDataGenerator:\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.results = {}\n",
    "        self.aiw_gt = []\n",
    "\n",
    "    def generate_questions(self, doc):\n",
    "        gt_prompt = gt_prompt_template.substitute(**doc)\n",
    "\n",
    "        response = self.llm.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=[{\"role\": \"user\", \"content\": gt_prompt}]\n",
    "        )\n",
    "\n",
    "        json_response = response.choices[0].message.content\n",
    "        return json_response\n",
    "    \n",
    "\n",
    "    def bulk_generate(self, chunks):\n",
    "      for doc in tqdm(chunks):\n",
    "          doc_id = doc['id']\n",
    "          if doc_id in self.results:\n",
    "              continue\n",
    "          try:\n",
    "              res = self.generate_questions(doc) or \"[]\"\n",
    "              self.results[doc['id']] = json.loads(res)\n",
    "          except Exception as e:\n",
    "              print(\"Error for doc:\", doc['id'], e)\n",
    "\n",
    "\n",
    "    def save(self, out_path=\"../DATA/GT/aiw_golden_data.json\"):\n",
    "        with open(out_path, \"w\") as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "        print(f\"Saved {len(self.results)} records to {out_path}\")\n",
    "\n",
    "\n",
    "    def load(self, in_path=\"../DATA/aiw_golden_data.json\"):\n",
    "      with open(\"../DATA/GT/aiw_golden_data.json\", \"r\") as f:\n",
    "          res = json.load(f)\n",
    "\n",
    "          for cid, queries in res.items():\n",
    "              for q in queries:\n",
    "                  self.aiw_gt.append(\n",
    "                      {\n",
    "                          'gold_id': cid,\n",
    "                          'query': q\n",
    "                      }\n",
    "                  )\n",
    "\n",
    "    def get_golden_data(self):\n",
    "        return self.aiw_gt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21d5d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = GoldenDataGenerator()\n",
    "# gen.generate_questions(chunks[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "663c30ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = GoldenDataGenerator()\n",
    "# gen.bulk_generate(chunks)\n",
    "# gen.save()  \n",
    "gen.load()\n",
    "aiw_gt = gen.get_golden_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4686c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'gold_id': 'aiw_01_001_de2f1cf', 'query': 'Mad Tea-Party chapter'},\n",
       " {'gold_id': 'aiw_01_001_de2f1cf',\n",
       "  'query': 'What happens in the Pool of Tears?'},\n",
       " {'gold_id': 'aiw_01_001_de2f1cf', 'query': 'Caucus-Race outcome'},\n",
       " {'gold_id': 'aiw_01_001_de2f1cf', 'query': 'Who sends in Bill?'},\n",
       " {'gold_id': 'aiw_01_001_de2f1cf',\n",
       "  'query': 'What does the tea party symbolize?'},\n",
       " {'gold_id': 'aiw_02_001_174ec36',\n",
       "  'query': \"Alice's curiosity about the Rabbit\"},\n",
       " {'gold_id': 'aiw_02_001_174ec36',\n",
       "  'query': \"Why did Alice think the Rabbit's actions were natural?\"},\n",
       " {'gold_id': 'aiw_02_001_174ec36', 'query': 'falling down the rabbit-hole'},\n",
       " {'gold_id': 'aiw_02_001_174ec36',\n",
       "  'query': 'Who does Alice sit with at the beginning?'},\n",
       " {'gold_id': 'aiw_02_001_174ec36',\n",
       "  'query': 'Theme of boredom in the opening scene'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiw_gt[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "903d32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of search quality\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "class SearchEvaluator:\n",
    "\n",
    "    def __init__(self, chunks, searcher) -> None:\n",
    "        self.chunks = chunks\n",
    "        self.searcher = searcher\n",
    "        self.results = []\n",
    "\n",
    "    @staticmethod\n",
    "    def hit_rate_at_k(res_ids, gold_id, k=5):\n",
    "        return 1.0 if gold_id in res_ids[:k] else 0.0\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def mrr_at_k(res_ids, gold_id, k=5):\n",
    "        for i, res_id in enumerate(res_ids[:k], start=1):\n",
    "            if res_id == gold_id:\n",
    "                return 1.0 / i \n",
    "        return 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_metrics(results, k_values=[5, 7]):\n",
    "\n",
    "        if not results:\n",
    "            return {f\"hit_rate_at_{k}\": 0.0 for k in k_values} | {f\"mrr_at_{k}\": 0.0 for k in k_values}\n",
    "        \n",
    "        metrics = defaultdict(list)\n",
    "\n",
    "        for result in results:\n",
    "            res_ids = result.get('chunk_ids')\n",
    "            gold_id = result.get('gold_id')\n",
    "\n",
    "            for k in k_values:\n",
    "                metrics[f'hit_rate_at_{k}'].append(SearchEvaluator.hit_rate_at_k(res_ids, gold_id, k))\n",
    "                metrics[f'mrr_at_{k}'].append(SearchEvaluator.mrr_at_k(res_ids, gold_id, k))\n",
    "\n",
    "        avg_metrics = {}\n",
    "        for metric, values in metrics.items():\n",
    "            avg_metrics[metric] = sum(values) / len(values) if values else 0.0\n",
    "\n",
    "        avg_metrics['total_queries'] = len(results)\n",
    "\n",
    "        return avg_metrics\n",
    "    \n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        results = []\n",
    "        self.searcher.build_index(self.chunks)\n",
    "\n",
    "        for item in aiw_gt:\n",
    "            gold_id = item['gold_id']\n",
    "            query = item['query']\n",
    "\n",
    "            res = self.searcher.id_search(query)\n",
    "            results.append(\n",
    "                {\n",
    "                    'gold_id': gold_id,\n",
    "                    'chunk_ids': res\n",
    "                }\n",
    "            )\n",
    "        metrics = SearchEvaluator.calculate_metrics(results)\n",
    "        return metrics\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d7a1488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"hit_rate_at_5\": 0.793939393939394,\n",
      "  \"mrr_at_5\": 0.5865319865319865,\n",
      "  \"hit_rate_at_7\": 0.8282828282828283,\n",
      "  \"mrr_at_7\": 0.5918229918229918,\n",
      "  \"total_queries\": 495\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "bm25 = BM25Retriever()\n",
    "bm25_evaluator = SearchEvaluator(chunks, bm25)\n",
    "bm25_metrics = bm25_evaluator.evaluate()\n",
    "print(json.dumps(bm25_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88fc6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Search \n",
    "from qdrant_client import QdrantClient, models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class SemanticRetriever:\n",
    "\n",
    "    COLLECTION = \"book_chunks\"\n",
    "\n",
    "    def __init__(self, transformer=\"BAAI/bge-small-en\") -> None:\n",
    "        super().__init__()\n",
    "        self.embedder = SentenceTransformer(transformer)\n",
    "        self.qdrant = QdrantClient(\"localhost\", port=6333)\n",
    "\n",
    "        self.embeddings = []\n",
    "        self.ids = []\n",
    "\n",
    "    def embed_batch(self, chunks):\n",
    "        texts = [c[\"text\"] for c in chunks]\n",
    "        vecs = self.embedder.encode(texts, normalize_embeddings=True).tolist()\n",
    "        return vecs\n",
    "\n",
    "\n",
    "    def build_index(self, chunks):\n",
    "\n",
    "        vectors = self.embed_batch(chunks)\n",
    "        print(\"Vector shape:\", len(vectors), len(vectors[0]))\n",
    "\n",
    "        if not self.qdrant.collection_exists(SemanticRetriever.COLLECTION):\n",
    "            self.qdrant.create_collection(\n",
    "                collection_name=SemanticRetriever.COLLECTION,\n",
    "                vectors_config=models.VectorParams(\n",
    "                    size=len(vectors[0]),\n",
    "                    distance=models.Distance.COSINE\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Upsert points\n",
    "        self.qdrant.upsert(\n",
    "            collection_name=SemanticRetriever.COLLECTION,\n",
    "            points=[\n",
    "                models.PointStruct(\n",
    "                    id=i,\n",
    "                    vector=vectors[i],\n",
    "                    payload=chunks[i]\n",
    "                )\n",
    "                for i in range(len(chunks))\n",
    "            ]\n",
    "        )\n",
    "        print(f\" ## Inserted: {len(chunks)} chunks into Qdrant\")\n",
    "\n",
    "    def search(self, query, topk=7):\n",
    "        vec = self.embedder.encode([query], normalize_embeddings=True)[0].tolist()\n",
    "        hits = self.qdrant.search(collection_name=SemanticRetriever.COLLECTION, query_vector=vec, limit=topk)\n",
    "        return [{\"id\": h.payload[\"id\"], \"text\": h.payload[\"text\"], \"score\": h.score} for h in hits]\n",
    "\n",
    "    def id_search(self, query: str, topk=7):\n",
    "        search_results = self.search(query, topk)\n",
    "        return [c[\"id\"] for c in search_results]\n",
    "\n",
    "    def cleanup(self):\n",
    "        self.embeddings = []\n",
    "        self.ids = []\n",
    "        self.qdrant.delete_collection(SemanticRetriever.COLLECTION)\n",
    "        print(\"Semantic index cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d7dc64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/dq11054j6sg0m5lt67ct6fr40000gq/T/ipykernel_98018/598598054.py:12: UserWarning: Qdrant client version 1.15.1 is incompatible with server version 1.8.3. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  self.qdrant = QdrantClient(\"localhost\", port=6333)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic index cleared.\n",
      "Vector shape: 99 384\n",
      " ## Inserted: 99 chunks into Qdrant\n",
      "Vector shape: 99 384\n",
      " ## Inserted: 99 chunks into Qdrant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/dq11054j6sg0m5lt67ct6fr40000gq/T/ipykernel_98018/598598054.py:53: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = self.qdrant.search(collection_name=SemanticRetriever.COLLECTION, query_vector=vec, limit=topk)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"hit_rate_at_5\": 0.5777777777777777,\n",
      "  \"mrr_at_5\": 0.40898989898989896,\n",
      "  \"hit_rate_at_7\": 0.6363636363636364,\n",
      "  \"mrr_at_7\": 0.4181770081770082,\n",
      "  \"total_queries\": 495\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "vec = SemanticRetriever()\n",
    "vec.cleanup()  # Clear previous index if any to avoid dimension mismatch\n",
    "vec.build_index(chunks)\n",
    "vec_evaluator = SearchEvaluator(chunks, vec)\n",
    "vec_metrics = vec_evaluator.evaluate()\n",
    "print(json.dumps(vec_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71991c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class FusionRetriever:\n",
    "\n",
    "    def __init__(self, transformer=\"BAAI/bge-small-en\", alpha=0.7) -> None:\n",
    "        self.bm25 = BM25Retriever()\n",
    "        self.vec = SemanticRetriever(transformer=transformer)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def build_index(self, chunks):\n",
    "        self.bm25.build_index(chunks)\n",
    "        self.vec.build_index(chunks)\n",
    "\n",
    "    @staticmethod\n",
    "    def rrf_fusion(bm25_results, embed_results, k=7, c=60):\n",
    "        \"\"\"\n",
    "        Fuse BM25 + Embedding rankings using Reciprocal Rank Fusion (RRF).\n",
    "        \n",
    "        Returns:\n",
    "            list of chunk_ids (top-k fused)\n",
    "        \"\"\"\n",
    "        ranks = defaultdict(float)\n",
    "\n",
    "        # BM25 contribution\n",
    "        for rank, chunk in enumerate(bm25_results, start=1):\n",
    "            ranks[chunk[\"id\"]] += 1.0 / (c + rank)\n",
    "\n",
    "        # Embedding contribution\n",
    "        for rank, chunk in enumerate(embed_results, start=1):\n",
    "            ranks[chunk[\"id\"]] += 1.0 / (c + rank)\n",
    "\n",
    "        # Sort by fused score\n",
    "        fused = sorted(ranks.items(), key=lambda x: -x[1])[:k]\n",
    "        return [cid for cid, _ in fused]\n",
    "\n",
    "    def weighted_fusion(self, bm25_results, embed_results, topk=7):\n",
    "        scores = defaultdict(float)\n",
    "        for rank, c in enumerate(bm25_results, start=1):\n",
    "            scores[c[\"id\"]] += self.alpha * (1.0 / rank)\n",
    "        for rank, c in enumerate(embed_results, start=1):\n",
    "            scores[c[\"id\"]] += (1 - self.alpha) * (1.0 / rank)\n",
    "        return [cid for cid, _ in sorted(scores.items(), key=lambda x: -x[1])[:topk]]\n",
    "    \n",
    "    def id_search(self, query: str, topk=7):\n",
    "        bm25_results = self.bm25.search(query, topk * 2)   \n",
    "        embed_results = self.vec.search(query, topk * 2) \n",
    "        return self.weighted_fusion(bm25_results, embed_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a6dcb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/dq11054j6sg0m5lt67ct6fr40000gq/T/ipykernel_98018/598598054.py:12: UserWarning: Qdrant client version 1.15.1 is incompatible with server version 1.8.3. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  self.qdrant = QdrantClient(\"localhost\", port=6333)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector shape: 99 384\n",
      " ## Inserted: 99 chunks into Qdrant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/dq11054j6sg0m5lt67ct6fr40000gq/T/ipykernel_98018/598598054.py:53: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = self.qdrant.search(collection_name=SemanticRetriever.COLLECTION, query_vector=vec, limit=topk)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"hit_rate_at_5\": 0.8101010101010101,\n",
      "  \"mrr_at_5\": 0.5944444444444444,\n",
      "  \"hit_rate_at_7\": 0.8585858585858586,\n",
      "  \"mrr_at_7\": 0.601996151996152,\n",
      "  \"total_queries\": 495\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "fusion_retriever = FusionRetriever()\n",
    "fusion_evaluator = SearchEvaluator(chunks, fusion_retriever)\n",
    "fusion_metrics = fusion_evaluator.evaluate()\n",
    "print(json.dumps(fusion_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ca028a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Search Metrics: {\n",
      "  \"hit_rate_at_5\": 0.793939393939394,\n",
      "  \"mrr_at_5\": 0.5865319865319865,\n",
      "  \"hit_rate_at_7\": 0.8282828282828283,\n",
      "  \"mrr_at_7\": 0.5918229918229918,\n",
      "  \"total_queries\": 495\n",
      "}\n",
      "Vector Search Metrics {\n",
      "  \"hit_rate_at_5\": 0.5777777777777777,\n",
      "  \"mrr_at_5\": 0.40898989898989896,\n",
      "  \"hit_rate_at_7\": 0.6363636363636364,\n",
      "  \"mrr_at_7\": 0.4181770081770082,\n",
      "  \"total_queries\": 495\n",
      "}\n",
      "Fusion Search Metrics {\n",
      "  \"hit_rate_at_5\": 0.8101010101010101,\n",
      "  \"mrr_at_5\": 0.5944444444444444,\n",
      "  \"hit_rate_at_7\": 0.8585858585858586,\n",
      "  \"mrr_at_7\": 0.601996151996152,\n",
      "  \"total_queries\": 495\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"BM25 Search Metrics:\" , json.dumps(bm25_metrics, indent=2))\n",
    "print(\"Vector Search Metrics\", json.dumps(vec_metrics, indent=2))\n",
    "print(\"Fusion Search Metrics\", json.dumps(fusion_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccc2a0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/dq11054j6sg0m5lt67ct6fr40000gq/T/ipykernel_98018/598598054.py:12: UserWarning: Qdrant client version 1.15.1 is incompatible with server version 1.8.3. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  self.qdrant = QdrantClient(\"localhost\", port=6333)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic index cleared.\n",
      "Vector shape: 99 768\n",
      " ## Inserted: 99 chunks into Qdrant\n",
      "Vector shape: 99 768\n",
      " ## Inserted: 99 chunks into Qdrant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/dq11054j6sg0m5lt67ct6fr40000gq/T/ipykernel_98018/598598054.py:53: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = self.qdrant.search(collection_name=SemanticRetriever.COLLECTION, query_vector=vec, limit=topk)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"hit_rate_at_5\": 0.6707070707070707,\n",
      "  \"mrr_at_5\": 0.4751515151515151,\n",
      "  \"hit_rate_at_7\": 0.7292929292929293,\n",
      "  \"mrr_at_7\": 0.48453102453102453,\n",
      "  \"total_queries\": 495\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "vec2 = SemanticRetriever(transformer=\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")\n",
    "vec2.cleanup()  # Clear previous index if any to avoid dimension mismatch\n",
    "vec2.build_index(chunks)\n",
    "vec2_evaluator = SearchEvaluator(chunks, vec2)\n",
    "vec2_metrics = vec2_evaluator.evaluate()\n",
    "print(json.dumps(vec2_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7afc7e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/dq11054j6sg0m5lt67ct6fr40000gq/T/ipykernel_98018/598598054.py:12: UserWarning: Qdrant client version 1.15.1 is incompatible with server version 1.8.3. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  self.qdrant = QdrantClient(\"localhost\", port=6333)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector shape: 99 768\n",
      " ## Inserted: 99 chunks into Qdrant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/dq11054j6sg0m5lt67ct6fr40000gq/T/ipykernel_98018/598598054.py:53: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = self.qdrant.search(collection_name=SemanticRetriever.COLLECTION, query_vector=vec, limit=topk)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"hit_rate_at_5\": 0.8141414141414142,\n",
      "  \"mrr_at_5\": 0.6090909090909091,\n",
      "  \"hit_rate_at_7\": 0.8444444444444444,\n",
      "  \"mrr_at_7\": 0.6138528138528138,\n",
      "  \"total_queries\": 495\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "fusion_retriever = FusionRetriever(transformer=\"sentence-transformers/multi-qa-mpnet-base-cos-v1\", alpha=0.5)\n",
    "fusion_evaluator = SearchEvaluator(chunks, fusion_retriever)\n",
    "fusion_metrics = fusion_evaluator.evaluate()\n",
    "print(json.dumps(fusion_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11d524e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/dq11054j6sg0m5lt67ct6fr40000gq/T/ipykernel_98018/598598054.py:12: UserWarning: Qdrant client version 1.15.1 is incompatible with server version 1.8.3. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  self.qdrant = QdrantClient(\"localhost\", port=6333)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector shape: 99 768\n",
      " ## Inserted: 99 chunks into Qdrant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/dq11054j6sg0m5lt67ct6fr40000gq/T/ipykernel_98018/598598054.py:53: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = self.qdrant.search(collection_name=SemanticRetriever.COLLECTION, query_vector=vec, limit=topk)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"hit_rate_at_5\": 0.8141414141414142,\n",
      "  \"mrr_at_5\": 0.6090909090909091,\n",
      "  \"hit_rate_at_7\": 0.8444444444444444,\n",
      "  \"mrr_at_7\": 0.6138528138528138,\n",
      "  \"total_queries\": 495\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "fusion_retriever = FusionRetriever(transformer=\"sentence-transformers/multi-qa-mpnet-base-cos-v1\", alpha=0.5)\n",
    "fusion_evaluator = SearchEvaluator(chunks, fusion_retriever)\n",
    "fusion_metrics = fusion_evaluator.evaluate()\n",
    "print(json.dumps(fusion_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c61a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book-mate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
